---
layout: post
title: 速習強化学習
tags: [Projects]
permalink: projects/2020-10-11-rl-sokushu
---

## 概要
速習強化学習は，強化学習におけるバイブル的な存在であるSutton本に対し，新たなアルゴリズムを加えつつ，非常に簡潔にまとめられたAlgorithms for Reinforcement Learningという本の訳本です．速習強化学習はその名の通り，100ページにまとめられていて"速習"が可能であり，今回は強化学習の学習経験のあるメンバーに初学の新たな参加者を加え輪読を行うこととなりました．そのため，前半は速習強化学習を輪読する形式で強化学習の数理的な基礎部分への理解を深め，後半は学習経験あるのメンバーが最近の手法 (DDPG, PPO, R2D3, Agent57など) について調べて発表を行うことで，参加者それぞれのレベルに応じて強化学習への理解を深める勉強会になっています．

## 目的
- 強化学習を基礎からしっかり理解する．
- 強化学習の最近の手法までの流れを掴む．

第1章:マルコフ決定過程 \
第2章:価値推定問題 \
第3章:制御 \
第4章:さらなる勉強のために

## 実施期間・日時
場所: オンライン (Zoom) \
日時: 2020年6月 - 10月, 毎週日曜日13時-15時

## 参考資料
[1] C. Szepesvari. 小山田 創哲 訳. 速習強化学習-基礎理論とアルゴリズム-. 共立出版, 2017. \
[2] V. Mnih et al. Playing Atari with Deep Reinforcement Learning. NeurIPS, 2013. [[arXiv]](https://arxiv.org/abs/1312.5602) \
[3] T. Schaul et al. Prioritized Experience Replay. ICLR, 2016. [[arXiv]](https://arxiv.org/abs/1511.05952) \
[4] H. Hasselt. Double Q-learning. NeurIPS, 2010. [[pdf]](https://papers.nips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf) \
[5] H. Hasselt et al. Deep Reinforcement Learning with Double Q-Learning. AAAI, 2016. [[arXiv]](https://arxiv.org/abs/1509.06461) \
[6] Z. Wang et al. Dueling Network Architectures for Deep Reinforcement Learning. ICML, 2016. [[arXiv]](https://arxiv.org/abs/1511.06581) \
[7] M. G. Bellemare et al. A Distributional Perspective on Reinforcement Learning. ICML, 2017. [[arXiv]](https://arxiv.org/abs/1707.06887) \
[8] M. Fortunato et al. Noisy Networks For Exploration. ICLR, 2018. [[pdf]](https://openreview.net/forum?id=rywHCPkAW) \
[9] M. Hessel et al. Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI, 2018. [[arXiv]](https://arxiv.org/abs/1710.02298) \
[10] A. Nair et al. Massively Parallel Methods for Deep Reinforcement Learning. ICML, 2015. [[arXiv]](https://arxiv.org/abs/1507.04296) \
[11] D. Horgan et al. Distributed Prioritized Experience Replay. ICLR, 2018. [[arXiv]](https://arxiv.org/abs/1803.00933) \
[12] S. Kapturowski et al. Recurrent Experience Replay in Distributed Reinforcement Learning. ICLR, 2019. [[pdf]](https://openreview.net/forum?id=r1lyTjAqYX) \
[13] C. Gulcehre et al. Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. ICLR, 2020. [[arXiv]](https://arxiv.org/abs/1909.01387) \
[14] A. Badia et al. Never Give Up: Learning Directed Exploration Strategies. ICLR, 2020. [[arXiv]](https://arxiv.org/abs/2002.06038) \
[15] A. P. Badia et al. Agent57: Outperforming the Atari Human Benchmark, ICML, 2020. [[arXiv]](https://arxiv.org/abs/2003.13350) \
[16] T. P. Lillicrap et al. Continuous Control with Deep Reinforcement Learning. ICLR, 2016. [[arXiv]](https://arxiv.org/abs/1509.02971) \
[17] J. Schulman et al. Trust Region Policy Optimization. ICML, 2015. [[arXiv]](https://arxiv.org/abs/1502.05477) \
[18] J. Schulman et al. Proximal Policy Optimization. 2017. [[arXiv]](https://arxiv.org/abs/1707.06347)