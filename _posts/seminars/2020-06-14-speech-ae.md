---
layout: post
title: 音声分野における敵対的摂動
tags: [Seminars]
permalink: seminars/2020-06-14-speech-ae
---

## 概要
近年，敵対的摂動という深層学習モデルの脆弱性を利用した攻撃手段が指摘され，それ以降，深層学習モデルに対する様々な攻撃手法及び防衛手法が指摘されてきました．敵対的摂動は深層学習モデルの挙動を改変するような摂動であり，かつ人間には知覚できない摂動であるため，深層学習の実世界への応用に際して人間には気づけない様々な問題を起こす可能性があり，その攻撃手段及び防衛手段の研究は重要な課題となっています．本セミナーでは，いくつかの攻撃手法及び防衛手法を紹介し，また，近年深層学習の応用が進む音声分野での敵対的摂動の論文を紹介します．

## 目的
- 敵対的摂動とは何かを理解する．
- 敵対的摂動を用いた攻撃手法及び防衛手法を知る．
- 音声分野における敵対的摂動を用いた攻撃手法を知る．

## 発表日時
場所:  オンライン (Zoom) \
日時: 2020年6月14日 10時 - 12時

## 参考資料
[1] C. Szegedy et al. Intriguing properties of neural networks. ICLR, 2014. [[arXiv]](https://arxiv.org/abs/1312.6199) \
[2] I. Goodfellow et al. Explaining and Harnessing Adversarial Examples. ICLR, 2015. [[arXiv]](https://arxiv.org/abs/1412.6572) \
[3] N. Papernot et al. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. S&P, 2016. [[arXiv]](https://arxiv.org/abs/1511.04508) \
[4] N. Carlini and D. Wagner. Towards Evaluating the Robustness of Neural Networks. S&P, 2017. [[arXiv]](https://arxiv.org/abs/1608.04644) \
[5] A. Athalye et al. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ICML, 2018. [[arXiv]](https://arxiv.org/abs/1802.00420) \
[6] N. Carlini and D. Wagner. Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. S&P, 2018. [[arXiv]](https://arxiv.org/abs/1801.01944) \
[7] Y. Qin et al. Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition. ICML, 2019. [[arXiv]](https://arxiv.org/abs/1903.10346)